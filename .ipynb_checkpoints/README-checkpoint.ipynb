{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Healthcare Fraud Using Machine Learning \n",
    "## 1. Background and Motivation\n",
    "The Institute of Medicine estimated that 30 percent of U.S. health spending (public and private) in 2009 — roughly `$750 billion — was wasted` on unnecessary services, excessive administrative costs, `fraud`, and other problems. And according to the 2016 Ernst and Young audit report of the US Department of Health and Human Services, HHS did not achieve an `improper payment rate` of less than `10 percent` for the Medicare Fee-for-Service and Medicaid programs. (sources: [Management Issue 5](https://oig.hhs.gov/reports-and-publications/top-challenges/2012/issue05.asp) and [2016 FY Audit Report](https://oig.hhs.gov/oas/reports/region17/171752000.asp))\n",
    "<br>\n",
    "<br>\n",
    "The primary motivation of this project is to explore the viability of using Machine Learning and Artificial Intelegence to detect potential fraud using publicly available data sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology and Technology Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources\n",
    "---\n",
    "<img src=\"images/data_sources.png\">\n",
    "<h4><center>Figure 1: Data Sources</center></h4>\n",
    "<br>\n",
    "There are plenty of data sources that can be incorporated into the Machine Learning model but given the time constraints, I decided to initially focus on:\n",
    "> Drug Prescription Data from Medicare as **predictors**, and <br>\n",
    "> List of Excluded Individuals and Entities as my **targets**.\n",
    "\n",
    "The notebook containing the exploratory data analysis into these data sets can be found [here](notebooks/EDA.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technology Stack\n",
    "---\n",
    "<img src=\"images/tech_stack.png\">\n",
    "<h4><center>Figure 2: Tech Stack</center></h4>\n",
    "<br>\n",
    "I used standard Python based Data Science tech stack. Also, given that I had large data set, I used a powerful EC2/Sagemaker instances on the AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preserving the Signal\n",
    "---\n",
    "<img src=\"images/number_of_records.png\">\n",
    "<h4><center>Figure 3: Imbalanced Classes</center></h4>\n",
    "\n",
    "<br>\n",
    "Generally, imbalanced classes is common in fraud detection. In other words, we'd expect to see far fewer fraudulent transactions relative to the non-fraudulent ones. In my case, the ratio is roughly 1-to-1000. \n",
    "<br>\n",
    "<br>\n",
    "A common strategy to balance out the labels is to either undersample the majority class or oversample the minority target class. I chose the latter and I used Random Bootstrap Sampling and SMOTE. The implementation of the code can be found [here](src/over_sample.py).\n",
    "<br>\n",
    "<br>\n",
    "Since a large number of excluded healthcare providers didn't have a National Provider Identifier, I tried matching the records by name, specialty, and state using fuzzy-wuzzy. Over 25 million pairs were compared but the results didn't yield in a significant contribution to address the `imbalanced classes` issue, so I didn't incorporate any of the resulting records into my model. You may still find the notebook with the implementation of fuzzy-wuzzy string matching [here](notebook/fuzzy-wuzzy.py).\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"images/exclusion_ratio.png\">\n",
    "<h4><center>Figure 4: Exclusion Ratio by State</center></h4>\n",
    "And finally, to maximize the presence of the target signal, i.e. the ratio of excluded providers relative to the total number of providers, I considered to limit the scope of the model only to the States with the highest exclusion ratios. The implementation of this strategy can be found in [this](notebook/exclusion_ratio.py) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Results\n",
    "---\n",
    "<img src=\"images/model_results.png\">\n",
    "<h4><center>Figure 5: Model Performance</center></h4>\n",
    "\n",
    "\n",
    "<br>\n",
    "Since my model had a significant class imbalance, to assess model performance I used Area Under the Curve (AUC). This is a common measure to evaluate the trade-off between correctly identifying the true signal relative to incorrectly identifying false signals as we increase the acceptance threshold.\n",
    "<br>\n",
    "<br>\n",
    "I initially ran the model through the `Random Forest` and the performance was:\n",
    "> AUC score: 0.72 <br>\n",
    "> Cross Validation score (mean): 0.61\n",
    "\n",
    "Next, I ran the model through other ensemble algorithms but the `Gradient Boosted Trees` came in first with:\n",
    "> AUC score: 0.78 <br>\n",
    "> Cross Validation score (mean): 0.63\n",
    "\n",
    "Also, the model picked up on these 2 features as most predective:\n",
    "> 30-day fill count <br>\n",
    "> total day supply\n",
    "\n",
    "And finally, the implementation of the model can be found in [this](notebook/model.py) notebook .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Next Steps\n",
    "---\n",
    "<img src=\"images/next_steps.png\">\n",
    "<h4><center>Figure 6: Next Steps</center></h4>\n",
    "<br>\n",
    "This is a very interestig topic. My next steps, will include incorporating more data sets in the hope of driving up model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
